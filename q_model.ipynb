{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from game2048 import Game2048\n",
    "from visuals import Visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN2048(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN2048, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2, stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(128* 2 * 2, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 4)  # Output layer: 4 possible actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional layers\n",
    "        x  = x.unsqueeze(1)        \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Linear output for Q-values\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=2000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            torch.tensor(action),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            torch.tensor(next_state, dtype=torch.float32),\n",
    "            torch.tensor(done, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 16  # 4x4 board flattened\n",
    "output_dim = 4  # 4 possible moves (up, down, left, right)\n",
    "#q_network = QNetwork(input_dim, output_dim)\n",
    "q_network = DQN2048()\n",
    "env = Game2048(flatten_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_network(env, q_network, episodes=1000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.HuberLoss()\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice([0, 1, 2, 3])\n",
    "            else:\n",
    "                q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward = env.move(['up', 'down', 'left', 'right'][action])\n",
    "            total_reward += reward\n",
    "            done = env.is_game_over()\n",
    "\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch_state, batch_action, batch_reward, batch_next_state, batch_done = replay_buffer.sample(batch_size)\n",
    "                \n",
    "                batch_state.requires_grad_(True)\n",
    "                \n",
    "                q_values = q_network(batch_state)\n",
    "                batch_action_unsqueezed = batch_action.unsqueeze(1).long()\n",
    "                q_value = q_values.gather(1, batch_action_unsqueezed).squeeze(1)\n",
    "                next_q_values = q_network(batch_next_state)\n",
    "                next_q_value = next_q_values.max(1)[0]\n",
    "\n",
    "                expected_q_value = batch_reward + gamma * next_q_value * (1 - batch_done)\n",
    "                \n",
    "                # Ensure expected_q_value does not require gradients\n",
    "                expected_q_value = expected_q_value.detach()\n",
    "                \n",
    "                loss = criterion(q_value, expected_q_value)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "train_q_network(env, q_network)\n",
    "torch.save(q_network.state_dict(), 'q_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = q_network\n",
    "loaded_model.load_state_dict(torch.load('q_network.pth'))\n",
    "q_network = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "visual = Visual()\n",
    "visual.model_play(q_network, env)\n",
    "#visual.play_game(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
